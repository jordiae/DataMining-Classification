{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age          job  marital            education  default housing loan  \\\n",
      "0   56    housemaid  married             basic.4y       no      no   no   \n",
      "1   59       admin.  married  professional.course       no      no   no   \n",
      "2   41  blue-collar  married              unknown  unknown      no   no   \n",
      "3   25     services   single          high.school       no     yes   no   \n",
      "4   29  blue-collar   single          high.school       no      no  yes   \n",
      "\n",
      "     contact month day_of_week      ...        pdays  previous     poutcome  \\\n",
      "0  telephone   may         mon      ...          999         0  nonexistent   \n",
      "1  telephone   may         mon      ...          999         0  nonexistent   \n",
      "2  telephone   may         mon      ...          999         0  nonexistent   \n",
      "3  telephone   may         mon      ...          999         0  nonexistent   \n",
      "4  telephone   may         mon      ...          999         0  nonexistent   \n",
      "\n",
      "  emp.var.rate  cons.price.idx  cons.conf.idx  euribor3m  nr.employed   y  \\\n",
      "0          1.1          93.994          -36.4      4.857       5191.0  no   \n",
      "1          1.1          93.994          -36.4      4.857       5191.0  no   \n",
      "2          1.1          93.994          -36.4      4.857       5191.0  no   \n",
      "3          1.1          93.994          -36.4      4.857       5191.0  no   \n",
      "4          1.1          93.994          -36.4      4.857       5191.0  no   \n",
      "\n",
      "  prev_contacted  \n",
      "0             no  \n",
      "1             no  \n",
      "2             no  \n",
      "3             no  \n",
      "4             no  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np    # Numeric and matrix computation\n",
    "import pandas as pd   # Optional: good package for manipulating data \n",
    "import sklearn as sk  # Package with learning algorithms implemented\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import warnings\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import preprocessing\n",
    "cv=50\n",
    "warnings.filterwarnings(\"ignore\", category=Warning)\n",
    "## Load dataset\n",
    "url_learn = \"../../../data/learning/BankCleanLearn.csv\"\n",
    "url_test = \"../../../data/test/BankCleanTest.csv\"\n",
    "data_learn = pd.read_csv(url_learn,sep=';')\n",
    "data_test = pd.read_csv(url_test,sep=';')\n",
    "print(data_learn.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def filterp(th, ProbClass1):\n",
    "        \"\"\" Given a treshold \"th\" and a set of probabilies of belonging to class 1 \"ProbClass1\", return predictions \"\"\"\n",
    "        y = ProbClass1.shape[0] * ['no']  # np.zeros(ProbClass1.shape[0])\n",
    "        for i, v in enumerate(ProbClass1):\n",
    "            if ProbClass1[i] > th:\n",
    "                y[i] = 'yes'\n",
    "        return y\n",
    "    # We do a 10 fold crossvalidation with 10 iterations\n",
    "    def compute_threshold(clf,Xx,Yy):\n",
    "        lth = []\n",
    "        kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "        for train_index, test_index in kf.split(Xx, Yy):\n",
    "            X_train2, X_test2 = Xx[train_index], Xx[test_index]\n",
    "            y_train2, y_test2 = Yy[train_index], Yy[test_index]\n",
    "\n",
    "            # Train with the training data of the iteration\n",
    "            clf.fit(X_train2, y_train2)\n",
    "            # Obtaining probablity predictions for test data of the iterarion\n",
    "            probs = clf.predict_proba(X_test2)\n",
    "            # Collect probabilities of belonging to class 1\n",
    "            ProbClass1 = probs[:, 1]\n",
    "            # Sort probabilities and generate pairs (threshold, f1-for-that-threshold)\n",
    "            res = np.array(\n",
    "                [[th, f1_score(y_test2, filterp(th, ProbClass1), pos_label='yes')] for th in np.sort(ProbClass1)])\n",
    "\n",
    "            # Uncomment the following lines if you want to plot at each iteration how f1-score evolves increasing the threshold\n",
    "            # plt.plot(res[:,0],res[:,1])\n",
    "            # plt.show()\n",
    "\n",
    "            # Find the threshold that has maximum value of f1-score\n",
    "            maxF = np.max(res[:, 1])\n",
    "            optimal_th = res[res[:, 1] == maxF, 0]\n",
    "            # Store the optimal threshold found for the current iteration\n",
    "            lth.append(optimal_th)\n",
    "        print(\"Thdef: \", np.mean(np.concatenate(lth, axis=0)))\n",
    "        return np.mean(np.concatenate(lth, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28832, 20)\n"
     ]
    }
   ],
   "source": [
    "## Separate data from labels\n",
    "X_learn=data_learn.drop(['y'], axis=1)\n",
    "y_learn=data_learn['y']\n",
    "\n",
    "print(X_learn.shape)\n",
    "X_learn.head()\n",
    "\n",
    "X_test=data_test.drop(['y'], axis=1)\n",
    "y_test=data_test['y']\n",
    "\n",
    "Xn_learn = pd.get_dummies(X_learn)\n",
    "Xn_test = pd.get_dummies(X_test)\n",
    "f_scorer = make_scorer(f1_score, pos_label='yes') #we will use f1_score as scoring metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score: 0.301 [Majority Voting]\n"
     ]
    }
   ],
   "source": [
    "#Best params knn: p = 3, neighbors: 29, 'weights': 'uniform'\n",
    "#Best params DT: min_samples_split=2, min_impurity_split=0.9, class_weight='balanced', criterion='entropy'\n",
    "#Best params GaussianNB??\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(Xn_learn)\n",
    "Xn_scaled = scaler.transform(Xn_learn)\n",
    "clf1 = GaussianNB()\n",
    "clf2 = KNeighborsClassifier(n_neighbors=29,weights='uniform', p=3)\n",
    "clf3 = DecisionTreeClassifier(criterion='entropy', min_samples_split=13, min_impurity_split=0.9, class_weight='balanced')\n",
    "\n",
    "eclf = VotingClassifier(estimators=[('nb', clf1), ('knn3', clf2), ('dt', clf3)], voting='hard')\n",
    "scores = cross_val_score(eclf, Xn_scaled, y_learn, cv=cv, scoring=f_scorer, n_jobs=-1)\n",
    "print(\"f1_score: %0.3f [%s]\" % (scores.mean() , \"Majority Voting\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score: 0.314 [Weighted Voting]\n",
      "Computing threshold...\n",
      "Training...\n",
      "Predicting...\n",
      "filter...\n",
      "report with learn dataset:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          no       0.94      0.91      0.92     25584\n",
      "         yes       0.44      0.56      0.49      3248\n",
      "\n",
      "   micro avg       0.87      0.87      0.87     28832\n",
      "   macro avg       0.69      0.74      0.71     28832\n",
      "weighted avg       0.89      0.87      0.88     28832\n",
      "\n",
      "Confusion Matrix: \n",
      "          pred:yes  pred:no\n",
      "true:yes      1829     1419\n",
      "true:no       2361    23223\n",
      "Report with Test Dataset:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          no       0.91      0.99      0.95     10964\n",
      "         yes       0.69      0.20      0.31      1392\n",
      "\n",
      "   micro avg       0.90      0.90      0.90     12356\n",
      "   macro avg       0.80      0.59      0.63     12356\n",
      "weighted avg       0.88      0.90      0.87     12356\n",
      "\n",
      "Confusion Matrix: \n",
      "          pred:yes  pred:no\n",
      "true:yes       277     1115\n",
      "true:no        122    10842\n"
     ]
    }
   ],
   "source": [
    "#Con pesos\n",
    "eclf = VotingClassifier(estimators=[('nb', clf1), ('knn3', clf2), ('dt', clf3)],voting='soft', weights=[1,2,2])\n",
    "scores = cross_val_score(eclf, Xn_scaled, y_learn, cv=cv, scoring=f_scorer, n_jobs=-1)\n",
    "print(\"f1_score: %0.3f [%s]\" % (scores.mean(), \"Weighted Voting\"))\n",
    "print(\"Computing threshold...\")\n",
    "#This is the best voting strategy, let's filter by threshold\n",
    "#thdef = compute_threshold(eclf,Xn_scaled,y_learn)\n",
    "thdef = 0.5531095381333833\n",
    "print(\"Training...\")\n",
    "eclf.fit(Xn_scaled,y_learn)\n",
    "\n",
    "print(\"Predicting...\")\n",
    "probs = eclf.predict_proba(Xn_scaled)\n",
    "# Generate predictions using probabilities and threshold found on 10 folds cross-validation\n",
    "print(\"filter...\")\n",
    "pred = filterp(thdef, probs[:, 1])\n",
    "print(\"report with learn dataset:\")\n",
    "# Print results with this prediction vector\n",
    "print(classification_report(y_learn, pred))\n",
    "print(\"Confusion Matrix: \")\n",
    "print(pd.DataFrame(confusion_matrix(y_learn, pred, labels=['yes', 'no']),\n",
    "                   index=['true:yes', 'true:no'], columns=['pred:yes', 'pred:no']))\n",
    "\n",
    "\n",
    "print(\"Report with Test Dataset:\")\n",
    "scaler = preprocessing.StandardScaler().fit(Xn_test)\n",
    "Xn_test_scaled = scaler.transform(Xn_test)\n",
    "probs = eclf.predict_proba(Xn_test_scaled)\n",
    "pred = filterp(thdef, probs[:, 1])\n",
    "print(classification_report(y_test, pred))\n",
    "print(\"Confusion Matrix: \")\n",
    "print(pd.DataFrame(confusion_matrix(y_test, pred, labels=['yes', 'no']),\n",
    "                   index=['true:yes', 'true:no'], columns=['pred:yes', 'pred:no']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working\n",
      "Thdef:  0.6348886090632861\n",
      "Threshold:  0.6348886090632861\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          no       0.95      0.90      0.92     25584\n",
      "         yes       0.44      0.59      0.50      3248\n",
      "\n",
      "   micro avg       0.87      0.87      0.87     28832\n",
      "   macro avg       0.69      0.75      0.71     28832\n",
      "weighted avg       0.89      0.87      0.88     28832\n",
      "\n",
      "With learning data:\n",
      "Confusion Matrix: \n",
      "          pred:yes  pred:no\n",
      "true:yes      1904     1344\n",
      "true:no       2447    23137\n",
      "With testing data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          no       0.95      0.91      0.93     10964\n",
      "         yes       0.45      0.60      0.51      1392\n",
      "\n",
      "   micro avg       0.87      0.87      0.87     12356\n",
      "   macro avg       0.70      0.75      0.72     12356\n",
      "weighted avg       0.89      0.87      0.88     12356\n",
      "\n",
      "With learning data:\n",
      "Confusion Matrix: \n",
      "          pred:yes  pred:no\n",
      "true:yes       830      562\n",
      "true:no       1027     9937\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "#best decision tree\n",
    "dtclass = DecisionTreeClassifier(min_samples_split=13, min_impurity_split=0.9, class_weight='balanced', criterion='entropy')\n",
    "\"\"\"\n",
    "lb=[]\n",
    "for nest in [1,2,5,10,20,50,100,200]:\n",
    "    scores = cross_val_score(BaggingClassifier(base_estimator=dtclass,n_estimators=nest), Xn_learn, y_learn, cv=cv, scoring=f_scorer, n_jobs=4)\n",
    "    print(\"f1_score: %0.3f [%s]\" % (scores.mean(), nest))\n",
    "    lb.append(scores.mean())\n",
    "\n",
    "lb2=[]    \n",
    "print()\n",
    "for nest in [1,2,5,10,20,50,100,200]:\n",
    "    scores = cross_val_score(BaggingClassifier(base_estimator=dtclass,n_estimators=nest,max_features=0.35), Xn_learn, y_learn, cv=cv, scoring=f_scorer, n_jobs=4)\n",
    "    print(\"f1_score: %0.3f [%s]\" % (scores.mean(), nest))\n",
    "    lb2.append(scores.mean())\n",
    "\"\"\"\n",
    "#Threshold\n",
    "print(\"working\")\n",
    "clf = BaggingClassifier(base_estimator=dtclass,n_estimators=100,max_features=0.35);\n",
    "clf.fit(Xn_learn,y_learn)\n",
    "#thdef = compute_threshold(clf, Xn_learn.as_matrix(),y_learn)\n",
    "thdef = 0.6348886090632861;\n",
    "probs = clf.predict_proba(Xn_learn)\n",
    "pred = filterp(thdef, probs[:, 1])\n",
    "print(\"With learning data:\")\n",
    "print(classification_report(y_learn, pred))\n",
    "print(\"Confusion Matrix: \")\n",
    "print(pd.DataFrame(confusion_matrix(y_learn, pred, labels=['yes', 'no']),\n",
    "                   index=['true:yes', 'true:no'], columns=['pred:yes', 'pred:no']))\n",
    "print(\"With testing data\")\n",
    "probs = clf.predict_proba(Xn_test)\n",
    "pred = filterp(thdef, probs[:, 1])\n",
    "print(classification_report(y_test, pred))\n",
    "print(\"Confusion Matrix: \")\n",
    "print(pd.DataFrame(confusion_matrix(y_test, pred, labels=['yes', 'no']),\n",
    "                   index=['true:yes', 'true:no'], columns=['pred:yes', 'pred:no']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working\n",
      "Thdef:  0.6864800523895827\n",
      "With learning data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          no       0.94      0.91      0.93     25584\n",
      "         yes       0.44      0.57      0.50      3248\n",
      "\n",
      "   micro avg       0.87      0.87      0.87     28832\n",
      "   macro avg       0.69      0.74      0.71     28832\n",
      "weighted avg       0.89      0.87      0.88     28832\n",
      "\n",
      "Confusion Matrix: \n",
      "          pred:yes  pred:no\n",
      "true:yes      1843     1405\n",
      "true:no       2328    23256\n",
      "With testing data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          no       0.95      0.91      0.93     10964\n",
      "         yes       0.45      0.58      0.51      1392\n",
      "\n",
      "   micro avg       0.87      0.87      0.87     12356\n",
      "   macro avg       0.70      0.75      0.72     12356\n",
      "weighted avg       0.89      0.87      0.88     12356\n",
      "\n",
      "Confusion Matrix: \n",
      "          pred:yes  pred:no\n",
      "true:yes       813      579\n",
      "true:no        976     9988\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#I will apply best params found on tree classifier\n",
    "lrf=[]\n",
    "\"\"\"\n",
    "for nest in [1,2,5,10,20,50,100,200]:\n",
    "    scores = cross_val_score(RandomForestClassifier(n_estimators=nest,min_samples_split=2, min_impurity_split=0.9, class_weight='balanced', criterion='entropy'), Xn_learn, y_learn, cv=cv, scoring=f_scorer, n_jobs=4)\n",
    "    print(\"f1_score: %0.3f [%s]\" % (scores.mean(), nest))\n",
    "    lrf.append(scores.mean())\n",
    "\"\"\"\n",
    "#Threshold\n",
    "print(\"working\")\n",
    "clf = RandomForestClassifier(n_estimators=100,min_samples_split=14, min_impurity_split=0.9, class_weight='balanced', criterion='entropy')\n",
    "clf.fit(Xn_learn,y_learn)\n",
    "thdef = compute_threshold(clf, Xn_learn.as_matrix(),y_learn)\n",
    "#thdef = 0.6864800523895827\n",
    "probs = clf.predict_proba(Xn_learn)\n",
    "pred = filterp(thdef, probs[:, 1])\n",
    "print(\"With learning data:\")\n",
    "print(classification_report(y_learn, pred))\n",
    "print(\"Confusion Matrix: \")\n",
    "print(pd.DataFrame(confusion_matrix(y_learn, pred, labels=['yes', 'no']),\n",
    "                   index=['true:yes', 'true:no'], columns=['pred:yes', 'pred:no']))\n",
    "print(\"With testing data\")\n",
    "probs = clf.predict_proba(Xn_test)\n",
    "pred = filterp(thdef, probs[:, 1])\n",
    "print(classification_report(y_test, pred))\n",
    "print(\"Confusion Matrix: \")\n",
    "print(pd.DataFrame(confusion_matrix(y_test, pred, labels=['yes', 'no']),\n",
    "                   index=['true:yes', 'true:no'], columns=['pred:yes', 'pred:no']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working\n",
      "Thdef:  0.688129670887999\n",
      "With learning data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          no       0.95      0.92      0.93     25584\n",
      "         yes       0.47      0.58      0.52      3248\n",
      "\n",
      "   micro avg       0.88      0.88      0.88     28832\n",
      "   macro avg       0.71      0.75      0.72     28832\n",
      "weighted avg       0.89      0.88      0.88     28832\n",
      "\n",
      "Confusion Matrix: \n",
      "          pred:yes  pred:no\n",
      "true:yes      1887     1361\n",
      "true:no       2143    23441\n",
      "With testing data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          no       0.94      0.92      0.93     10964\n",
      "         yes       0.46      0.57      0.51      1392\n",
      "\n",
      "   micro avg       0.88      0.88      0.88     12356\n",
      "   macro avg       0.70      0.74      0.72     12356\n",
      "weighted avg       0.89      0.88      0.88     12356\n",
      "\n",
      "Confusion Matrix: \n",
      "          pred:yes  pred:no\n",
      "true:yes       787      605\n",
      "true:no        914    10050\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "lext=[]\n",
    "\"\"\"\n",
    "for nest in [1,2,5,10,20,50,100,200]:\n",
    "    scores = cross_val_score(ExtraTreesClassifier(n_estimators=nest,min_samples_split=2, min_impurity_split=0.9, class_weight='balanced', criterion='entropy'), Xn_learn, y_learn, cv=cv, scoring=f_scorer,n_jobs=4)\n",
    "    print(\"f1_score: %0.3f [%s]\" % (scores.mean(), nest))\n",
    "    lext.append(scores.mean())\n",
    "\"\"\"\n",
    "#best nest: 20\n",
    "#Threshold\n",
    "print(\"working\")\n",
    "clf = ExtraTreesClassifier(n_estimators=100,min_samples_split=13, min_impurity_split=0.9, class_weight='balanced', criterion='entropy')\n",
    "clf.fit(Xn_learn,y_learn)\n",
    "thdef = compute_threshold(clf, Xn_learn.as_matrix(),y_learn)\n",
    "#thdef = 0.6348886090632861;\n",
    "probs = clf.predict_proba(Xn_learn)\n",
    "pred = filterp(thdef, probs[:, 1])\n",
    "print(\"With learning data:\")\n",
    "print(classification_report(y_learn, pred))\n",
    "print(\"Confusion Matrix: \")\n",
    "print(pd.DataFrame(confusion_matrix(y_learn, pred, labels=['yes', 'no']),\n",
    "                   index=['true:yes', 'true:no'], columns=['pred:yes', 'pred:no']))\n",
    "print(\"With testing data\")\n",
    "probs = clf.predict_proba(Xn_test)\n",
    "pred = filterp(thdef, probs[:, 1])\n",
    "print(classification_report(y_test, pred))\n",
    "print(\"Confusion Matrix: \")\n",
    "print(pd.DataFrame(confusion_matrix(y_test, pred, labels=['yes', 'no']),\n",
    "                   index=['true:yes', 'true:no'], columns=['pred:yes', 'pred:no']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thdef:  0.49622207696143633\n",
      "With learning data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          no       0.94      0.91      0.92     25584\n",
      "         yes       0.43      0.56      0.49      3248\n",
      "\n",
      "   micro avg       0.87      0.87      0.87     28832\n",
      "   macro avg       0.69      0.73      0.71     28832\n",
      "weighted avg       0.88      0.87      0.88     28832\n",
      "\n",
      "Confusion Matrix: \n",
      "          pred:yes  pred:no\n",
      "true:yes      1824     1424\n",
      "true:no       2377    23207\n",
      "With testing data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          no       0.94      0.91      0.93     10964\n",
      "         yes       0.45      0.58      0.50      1392\n",
      "\n",
      "   micro avg       0.87      0.87      0.87     12356\n",
      "   macro avg       0.70      0.75      0.72     12356\n",
      "weighted avg       0.89      0.87      0.88     12356\n",
      "\n",
      "Confusion Matrix: \n",
      "          pred:yes  pred:no\n",
      "true:yes       810      582\n",
      "true:no       1007     9957\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\"\"\"\n",
    "lboo=[]\n",
    "for nest in [1,2,5,10,20,50,100,200]:\n",
    "    scores = cross_val_score(AdaBoostClassifier(n_estimators=nest, random_state=1234), Xn_learn, y_learn, cv=cv, scoring=f_scorer, n_jobs=4)\n",
    "    print(\"f1_score: %0.3f [%s]\" % (scores.mean(), nest))\n",
    "    lboo.append(scores.mean())\n",
    "\"\"\"\n",
    "clf = AdaBoostClassifier(n_estimators=100, random_state=1234)\n",
    "clf.fit(Xn_learn,y_learn)\n",
    "\n",
    "thdef = compute_threshold(clf, Xn_learn.as_matrix(),y_learn)\n",
    "probs = clf.predict_proba(Xn_learn)\n",
    "pred = filterp(thdef, probs[:, 1])\n",
    "print(\"With learning data:\")\n",
    "print(classification_report(y_learn, pred))\n",
    "print(\"Confusion Matrix: \")\n",
    "print(pd.DataFrame(confusion_matrix(y_learn, pred, labels=['yes', 'no']),\n",
    "                   index=['true:yes', 'true:no'], columns=['pred:yes', 'pred:no']))\n",
    "print(\"With testing data\")\n",
    "probs = clf.predict_proba(Xn_test)\n",
    "pred = filterp(thdef, probs[:, 1])\n",
    "print(classification_report(y_test, pred))\n",
    "print(\"Confusion Matrix: \")\n",
    "print(pd.DataFrame(confusion_matrix(y_test, pred, labels=['yes', 'no']),\n",
    "                   index=['true:yes', 'true:no'], columns=['pred:yes', 'pred:no']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thdef:  0.5025362965965401\n",
      "With learning data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          no       0.95      0.95      0.95     25584\n",
      "         yes       0.64      0.63      0.63      3248\n",
      "\n",
      "   micro avg       0.92      0.92      0.92     28832\n",
      "   macro avg       0.79      0.79      0.79     28832\n",
      "weighted avg       0.92      0.92      0.92     28832\n",
      "\n",
      "Confusion Matrix: \n",
      "          pred:yes  pred:no\n",
      "true:yes      2041     1207\n",
      "true:no       1167    24417\n",
      "With testing data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          no       0.93      0.93      0.93     10964\n",
      "         yes       0.43      0.42      0.42      1392\n",
      "\n",
      "   micro avg       0.87      0.87      0.87     12356\n",
      "   macro avg       0.68      0.67      0.67     12356\n",
      "weighted avg       0.87      0.87      0.87     12356\n",
      "\n",
      "Confusion Matrix: \n",
      "          pred:yes  pred:no\n",
      "true:yes       579      813\n",
      "true:no        775    10189\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "#adaboosting using DecisionTreeClassifier, maybe change max_depth?\n",
    "\"\"\"\n",
    "lboodt=[]\n",
    "for nest in [1,2,5,10,20,50,100,200]:\n",
    "    scores = cross_val_score(AdaBoostClassifier(DecisionTreeClassifier(max_depth=5,min_samples_split=2, min_impurity_split=0.9, class_weight='balanced', criterion='entropy'),n_estimators=nest, random_state=1234), Xn_learn, y_learn, cv=cv, scoring=f_scorer, n_jobs=4)\n",
    "    print(\"f1_score: %0.3f [%s]\" % (scores.mean(), nest))\n",
    "    lboodt.append(scores.mean())\n",
    "\"\"\"   \n",
    "clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=5,min_samples_split=13, min_impurity_split=0.9, class_weight='balanced', criterion='entropy'),n_estimators=100, random_state=1234)\n",
    "clf.fit(Xn_learn,y_learn)\n",
    "\n",
    "thdef = compute_threshold(clf, Xn_learn.as_matrix(),y_learn)\n",
    "probs = clf.predict_proba(Xn_learn)\n",
    "pred = filterp(thdef, probs[:, 1])\n",
    "print(\"With learning data:\")\n",
    "print(classification_report(y_learn, pred))\n",
    "print(\"Confusion Matrix: \")\n",
    "print(pd.DataFrame(confusion_matrix(y_learn, pred, labels=['yes', 'no']),\n",
    "                   index=['true:yes', 'true:no'], columns=['pred:yes', 'pred:no']))\n",
    "print(\"With testing data\")\n",
    "probs = clf.predict_proba(Xn_test)\n",
    "pred = filterp(thdef, probs[:, 1])\n",
    "print(classification_report(y_test, pred))\n",
    "print(\"Confusion Matrix: \")\n",
    "print(pd.DataFrame(confusion_matrix(y_test, pred, labels=['yes', 'no']),\n",
    "                   index=['true:yes', 'true:no'], columns=['pred:yes', 'pred:no']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thdef:  0.20819411135401691\n",
      "With learning data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          no       0.94      0.92      0.93     25584\n",
      "         yes       0.46      0.56      0.51      3248\n",
      "\n",
      "   micro avg       0.88      0.88      0.88     28832\n",
      "   macro avg       0.70      0.74      0.72     28832\n",
      "weighted avg       0.89      0.88      0.88     28832\n",
      "\n",
      "Confusion Matrix: \n",
      "          pred:yes  pred:no\n",
      "true:yes      1827     1421\n",
      "true:no       2145    23439\n",
      "With testing data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          no       0.94      0.92      0.93     10964\n",
      "         yes       0.46      0.57      0.51      1392\n",
      "\n",
      "   micro avg       0.88      0.88      0.88     12356\n",
      "   macro avg       0.70      0.74      0.72     12356\n",
      "weighted avg       0.89      0.88      0.88     12356\n",
      "\n",
      "Confusion Matrix: \n",
      "          pred:yes  pred:no\n",
      "true:yes       794      598\n",
      "true:no        916    10048\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\"\"\"\n",
    "lgbboo=[]\n",
    "for nest in [1,2,5,10,20,50,100,200]:\n",
    "    scores = cross_val_score(GradientBoostingClassifier(n_estimators=nest), Xn_learn, y_learn, cv=cv, scoring=f_scorer, n_jobs=4)\n",
    "    print(\"f1_score: %0.3f [%s]\" % (scores.mean(), nest))\n",
    "    lgbboo.append(scores.mean())\n",
    "\"\"\"\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=100)\n",
    "clf.fit(Xn_learn,y_learn)\n",
    "\n",
    "thdef = compute_threshold(clf, Xn_learn.as_matrix(),y_learn)\n",
    "probs = clf.predict_proba(Xn_learn)\n",
    "pred = filterp(thdef, probs[:, 1])\n",
    "print(\"With learning data:\")\n",
    "print(classification_report(y_learn, pred))\n",
    "print(\"Confusion Matrix: \")\n",
    "print(pd.DataFrame(confusion_matrix(y_learn, pred, labels=['yes', 'no']),\n",
    "                   index=['true:yes', 'true:no'], columns=['pred:yes', 'pred:no']))\n",
    "print(\"With testing data\")\n",
    "probs = clf.predict_proba(Xn_test)\n",
    "pred = filterp(thdef, probs[:, 1])\n",
    "print(classification_report(y_test, pred))\n",
    "print(\"Confusion Matrix: \")\n",
    "print(pd.DataFrame(confusion_matrix(y_test, pred, labels=['yes', 'no']),\n",
    "                   index=['true:yes', 'true:no'], columns=['pred:yes', 'pred:no']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import  matplotlib.pyplot as plt\n",
    "\n",
    "# plt.figure(figsize=(12,6))\n",
    "# plt.plot([1,2,5,10,20,50,100,200],lb,label=\"Bagging DT\")\n",
    "# plt.plot([1,2,5,10,20,50,100,200],lb2,label=\"Bagging DT forced variance\")\n",
    "# plt.plot([1,2,5,10,20,50,100,200],lrf,label=\"Random Forest\")\n",
    "# plt.plot([1,2,5,10,20,50,100,200],lext,label=\"Extra Trees\")\n",
    "# plt.plot([1,2,5,10,20,50,100,200],lboo,label=\"AdaBoost Dec.Stumps\")\n",
    "# plt.plot([1,2,5,10,20,50,100,200],lboodt,label=\"AdaBoost DT\")\n",
    "# plt.plot([1,2,5,10,20,50,100,200],lgbboo,label=\"Gradient Boosting\")\n",
    "# plt.legend(loc='lower right')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection with Forests of trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.98520422e-02 1.48686986e-02 3.27405183e-02 1.56208736e-02\n",
      " 1.95178296e-01 4.86995688e-02 5.44627341e-02 1.94640932e-01\n",
      " 1.33998377e-01 3.40430787e-03 4.94852057e-03 1.74937755e-03\n",
      " 1.29542523e-03 1.92689248e-03 3.00635210e-03 1.49963333e-03\n",
      " 2.75975399e-03 3.05321663e-03 2.24239161e-03 9.89940860e-04\n",
      " 3.19341494e-04 2.37691934e-03 3.22326575e-03 3.71044347e-03\n",
      " 3.86940183e-04 2.01399170e-03 2.00138766e-03 3.22625794e-03\n",
      " 2.44973278e-03 5.32830641e-05 2.30633059e-03 3.11347543e-03\n",
      " 1.54768018e-03 9.78090704e-03 8.46963515e-03 0.00000000e+00\n",
      " 2.98293083e-03 7.53582552e-04 2.70378327e-03 2.57399673e-03\n",
      " 8.70243196e-04 2.13053451e-03 1.59569201e-02 1.53569678e-02\n",
      " 5.05812535e-03 8.59205568e-04 1.35193308e-04 3.96334698e-04\n",
      " 2.92156664e-03 9.14321138e-03 1.16487612e-02 6.24465950e-04\n",
      " 6.17764691e-03 8.41344306e-04 3.68136781e-03 3.74742492e-03\n",
      " 3.59761885e-03 2.70205004e-03 3.00450894e-03 2.56995221e-03\n",
      " 1.91534703e-02 2.66014282e-02 2.81335974e-02 3.37563213e-02]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100,min_samples_split=13, min_impurity_split=0.9, class_weight='balanced', criterion='entropy')\n",
    "clf = clf.fit(Xn_learn, y_learn)\n",
    "print(clf.feature_importances_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28832, 4)\n",
      "0.17732878143994102\n",
      "0.023304404730164503\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "#threshold should be the same as knn?\n",
    "model = SelectFromModel(clf, prefit=True, threshold=0.05)\n",
    "X_new = model.transform(Xn_learn)\n",
    "print(X_new.shape)\n",
    "\n",
    "print(np.mean(cross_val_score(KNeighborsClassifier(p = 3, n_neighbors= 29, weights= 'uniform'), X=Xn_learn, y=y_learn, cv=cv, scoring=f_scorer,n_jobs=4)))\n",
    "print(np.mean(cross_val_score(KNeighborsClassifier(p = 3, n_neighbors= 29, weights= 'uniform'), X=X_new, y=y_learn, cv=cv, scoring=f_scorer,n_jobs=4)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
